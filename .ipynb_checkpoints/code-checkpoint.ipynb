{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94ae7c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['StopWords_Auditor.txt',\n",
       " 'StopWords_Currencies.txt',\n",
       " 'StopWords_DatesandNumbers.txt',\n",
       " 'StopWords_Generic.txt',\n",
       " 'StopWords_GenericLong.txt',\n",
       " 'StopWords_Geographic.txt',\n",
       " 'StopWords_Names.txt']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "stop_words_dir = \"E:\\Projects\\Blackcoffer Internship Assignment\\StopWords\"\n",
    "\n",
    "stop_word_files = os.listdir(stop_words_dir)\n",
    "\n",
    "stop_word_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "144f524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list()\n",
    "for file in stop_word_files:\n",
    "    f = os.path.join(stop_words_dir, file)\n",
    "    with open(f,'r') as file_content:\n",
    "        stop_words+=file_content.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a37bfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(stop_words)):\n",
    "    stop_words[i] = stop_words[i].replace('\\n','').replace(' ', '').lower()\n",
    "    if '|' in stop_words[i]:\n",
    "        temp = stop_words[i]\n",
    "        temp = temp.split('|')\n",
    "        stop_words[i] = temp[0]\n",
    "        stop_words.append(temp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1cf518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words_dir = \"E:\\Projects\\Blackcoffer Internship Assignment\\MasterDictionary\\\\negative-words.txt\"\n",
    "\n",
    "positive_words_dir = \"E:\\Projects\\Blackcoffer Internship Assignment\\MasterDictionary\\positive-words.txt\"\n",
    "\n",
    "positive_words_list = list()\n",
    "negative_words_list = list()\n",
    "\n",
    "with open(negative_words_dir,'r') as f:\n",
    "    negative_words_list+=f.readlines()\n",
    "    \n",
    "for i in range(len(negative_words_list)):\n",
    "    negative_words_list[i] = negative_words_list[i].replace('\\n','').replace(' ', '').lower()\n",
    "    \n",
    "with open(positive_words_dir,'r') as f:\n",
    "    positive_words_list+=f.readlines()\n",
    "    \n",
    "for i in range(len(positive_words_list)):\n",
    "    positive_words_list[i] = positive_words_list[i].replace('\\n','').replace(' ', '').lower()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46c06c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "\n",
    "def get_tokens(input_text):\n",
    "    # Tokenize the input text\n",
    "    tokens = nltk.word_tokenize(input_text.lower())\n",
    "\n",
    "    # Remove stop words from the tokenized text\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "#     # Join the filtered tokens back into a string\n",
    "#     filtered_text = \" \".join(filtered_tokens).translate(str.maketrans('', '', string.punctuation))\n",
    "    return filtered_tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "13daca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_articles_dict = dict()\n",
    "\n",
    "articles_dir = \"E:\\Projects\\Blackcoffer Internship Assignment\\\\article_files\"\n",
    "article_files = os.listdir(articles_dir)\n",
    "\n",
    "for file in article_files:\n",
    "    id = file[:-4]\n",
    "    file = os.path.join(articles_dir, file) \n",
    "    with open(file, 'r',encoding = \"utf-8\") as f:\n",
    "        article = f.read()\n",
    "    id_articles_dict[id] = article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a409beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_tokens_dict = dict()\n",
    "\n",
    "token_count_dict = dict()\n",
    "\n",
    "for id, article in id_articles_dict.items():\n",
    "    id_tokens_dict[id] = get_tokens(article)\n",
    "    token_count_dict[id] = len(id_tokens_dict[id])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fb2f4819",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(id_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ad4e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_score_dict = dict()\n",
    "\n",
    "for id, tokens in id_tokens_dict.items():\n",
    "    positive_score_dict[id] = 0\n",
    "    for token in tokens:\n",
    "        if token in positive_words_list:\n",
    "            positive_score_dict[id] +=1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bba369cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_score_dict = dict()\n",
    "\n",
    "for id, tokens in id_tokens_dict.items():\n",
    "    negative_score_dict[id] = 0\n",
    "    for token in tokens:\n",
    "        if token in negative_words_list:\n",
    "            negative_score_dict[id] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "155895af",
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity_score_dict = dict()\n",
    "\n",
    "subjectivity_score_dict = dict()\n",
    "\n",
    "for id in id_tokens_dict.keys():\n",
    "    p = positive_score_dict[id]\n",
    "    n = negative_score_dict[id]\n",
    "    total = len(id_tokens_dict[id])\n",
    "    polarity_score_dict[id] = (p-n)/(p+n+0.000001)\n",
    "    subjectivity_score_dict[id] = (p+n)/(total+0.000001)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5991c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sentence_length_dict = dict()\n",
    "tokenizer = nltk.tokenize.sent_tokenize\n",
    "for id, article in id_articles_dict.items():\n",
    "    sentences = tokenizer(article)\n",
    "    sen_count = len(sentences)\n",
    "    words = len(article.split())\n",
    "    avg_sentence_length_dict[id] = words/sen_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "65d41c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\Rudrransh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('cmudict')\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "# Load the CMU pronunciation dictionary\n",
    "cmu_dict = cmudict.dict()\n",
    "\n",
    "# Function to check if a word is complex\n",
    "def is_complex(word):\n",
    "    num_syllables = 0\n",
    "    # Use regular expressions to remove non-alphabetic characters\n",
    "    word = re.sub(r'[^a-zA-Z]', '', word)\n",
    "    # Use the CMU pronunciation dictionary to get the number of syllables in the word\n",
    "    if word.lower() in cmu_dict:\n",
    "        num_syllables = len(list(y for y in cmu_dict[word.lower()][0] if y[-1].isdigit()))\n",
    "    return num_syllables >= 3\n",
    "\n",
    "\n",
    "complex_word_percent_dict = dict()\n",
    "\n",
    "complex_word_count_dict = dict()\n",
    "\n",
    "for id, article in id_articles_dict.items():\n",
    "    words = article.split()\n",
    "    # Create a list of complex words\n",
    "    complex_words = [word for word in words if is_complex(word)]\n",
    "\n",
    "    # Get the number of complex words\n",
    "    num_complex_words = len(complex_words)\n",
    "    complex_word_count_dict[id] = num_complex_words\n",
    "    complex_word_percent_dict[id] = num_complex_words*100/len(words)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "975dba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "fog_index_dict = dict()\n",
    "\n",
    "for id,article in id_articles_dict.items():\n",
    "    fog_index_dict[id] = 0.4 * (avg_sentence_length_dict[id] + complex_word_percent_dict[id])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7916d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_words_per_sentence = avg_sentence_length_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca5fba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyphen\n",
    "\n",
    "syllable_count_per_word = dict()\n",
    "\n",
    "# download the english language data if not already downloaded\n",
    "pyphen.language_fallback('en_US')\n",
    "\n",
    "# create a Pyphen object for English language\n",
    "dic = pyphen.Pyphen(lang='en_US')\n",
    "\n",
    "\n",
    "for id, article in id_articles_dict.items():\n",
    "    words = article.split()\n",
    "    num_syllables = 0\n",
    "    for word in words:\n",
    "        num_syllables+= len(dic.hyphenate(word))\n",
    "    syllable_count_per_word[id] = num_syllables/len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dde77b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
